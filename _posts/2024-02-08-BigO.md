---
title: Big O Notation
date: 2024-02-09 00:23
categories: [Programming, Algorithm]
tags: [algorithm, programming, math] # TAG names should always be lowercase
math: true
image: assets/img/algorithm/Background.jpg
---

In this post, we'll explore the concept of Big O notation, common time complexities, and visualize how these complexities grow as the input size increases.

## Definition

Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. In computer science, it is used to analyze the efficiency of algorithms and estimate their worst-case time complexity.

### Formal Definition

_Let $f(n)$ and $g(n)$ be functions mapping positive integers to positive real numbers. We say that $f(n)$ is $O(g(n))$, read as "f of n is big O of g of n", if and only if there exist positive constants $c$ and $n_0$ such that:_

_for all $n \geq n_0$, $$f(n) \leq c \cdot g(n)$$_

This means that $f(n) = O(g(n))$ indicates that the growth rate of $f(n)$ is bounded above by the growth rate of $g(n)$ up to a constant factor, for sufficiently large $n$. This notation is used to classify algorithms according to their running time or space requirements in the worst-case scenario.

## Common Time Complexities

![runtime_complexity](assets/img/algorithm/runtime_table.jpg)
