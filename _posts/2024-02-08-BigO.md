---
title: Big O Notation
date: 2024-02-09 00:23
categories: [Programming, Algorithm]
tags: [algorithm, programming, math, c++] # TAG names should always be lowercase
math: true
image: assets/img/algorithm/Background.jpg
---

In this post, we'll explore the concept of Big O notation, common time complexities, and visualize how these complexities grow as the input size increases.

## Introduction

### Time and Space Complexity

Algorithms are crucial part of any program, and good a programmer chooses the most efficient algorithm to solve a problem.

To analyze the efficiency of an algorithm, there are two big considerations:

1. Time Complexity - a measure of the amount of time an algorithm takes to run, in relation to the size of the input data.

2. Sapce Complexity - a measure of the amount of computer memory (or space) an algorithm needs to run to completion.

### Why do algorithms matter?

_Example_

Consider a simple problem: calculating the sum of all numbers up to an integer n.

$$
\sum_{i=1}^{n} i = \frac{n(n + 1)}{2}
$$

Solution 1: Efficient Algorithm

```python
def sum(n):
    return (n * (n+1)) / 2
```

Solution 2: Inefficient Algorithm

```python
def sum(n):
    sum = 0
    for i in range(1, n+1):
        sum += i

    return sum
```

The first solution does not take any memory space as there is no variable assignment involved in the solution.
Also, regardless of how large the input _n_ is, the code will just execute a simple mathematical expression.

On the other hand, the second solution requires a memory space to save the variable _sum_, and it executes the addition inside the for loop _n_ times.

The difference between the performances of the two solutions might be subtle when _n_ is small, yet as _n_ becomes larger, the difference will grow and be noticeable.

Choosing a good algorithm is a must to optimize the performance and efficiency of a program.
In the real world, poor efficiency will result in taking up more resources, and thus, more cost.

`Big O Notation` enables programmers to analyze the complexity and the efficiency of an algorithm.

## Definition

Big O notation is a mathematical notation that describes the limiting behavior of a function when the argument tends towards a particular value or infinity. In computer science, it is used to analyze the efficiency of algorithms and estimate their worst-case time complexity.

### Formal Definition

_Let $f(n)$ and $g(n)$ be functions mapping positive integers to positive real numbers. We say that $f(n)$ is $O(g(n))$, read as "f of n is big O of g of n", if and only if there exist positive constants $c$ and $n_0$ such that:_

_for all $n \geq n_0$, $$f(n) \leq c \cdot g(n)$$_

This means that $f(n) = O(g(n))$ indicates that the growth rate of $f(n)$ is bounded above by the growth rate of $g(n)$ up to a constant factor, for sufficiently large $n$. This notation is used to classify algorithms according to their running time or space requirements in the worst-case scenario.

## Common Time Complexities

![runtime_complexity](assets/img/algorithm/runtime_table.jpg)

Common Time Complexities include the following:

1. Constant time
2. Linear time
3. Logarithmic time
4. Linearithmic time
5. Quadratic time
6. Exponential time

### $O({1})$ - Constant Time

$O({1})$ represents algorithms that always execute in the same time (or space) regardless of the size of the input data.
Example: Accessing a specific element in an array.

```cpp
#include <iostream>

int main(){
    int myArray[] = {1, 2, 3, 4, 5};

    std::cout << myArray[0]; // returns the first element regardless of the size of the array

    return 0;
}
```

### $O(n)$ - Linear Time

$O(n)$ represents algorithms whose performance grows linearly and in direct proportion to the size of the input data set.
Example: Traversing an array.

```cpp
#include<iostream>

int main(){
    int myArray[] = {1, 2, 3, 4, 5};

    for(int i = 0; i < 5; i++) {
        std::cout << myArray[i]; // prints each element in the array
    }

    return 0;
}

```

### $O(\log n)$ - Logarithmic Time

Logarithmic time complexity $O(\log n)$ describes an algorithm that reduces the size of its input data by a significant fraction (usually half) with each step, leading to fewer steps as the input size grows.

Example: Binary search in a sorted array.

```cpp
#include <bits/stdc++.h>
using namespace std;

int binarySearch(int arr[], int l, int r, int x)
{
    while (l <= r) {
        int m = l + (r - l) / 2;

        if (arr[m] == x)
            return m;
        if (arr[m] < x)
            l = m + 1;
        else
            r = m - 1;
    }
    return -1;
}

int main(void)
{
    int arr[] = { 2, 3, 4, 10, 40 };
    int x = 10;
    int n = sizeof(arr) / sizeof(arr[0]);
    int result = binarySearch(arr, 0, n - 1, x);
    (result == -1)
        ? cout << "Element is not present in array"
        : cout << "Element is present at index " << result;
    return 0;
}
```

### $O(n\log n)$ - Linearithmic Time

Linearithmic time complexity $O(n\log n)$ represents algorithms where the time grows in proportion to $n \log n$, combining linear and logarithmic behavior. This complexity is common in efficient sorting algorithms.

Example: Merge Sort algorithm.

```cpp
#include <bits/stdc++.h>
using namespace std;

void merge(int array[], int const left, int const mid,
		int const right)
{
	int const subArrayOne = mid - left + 1;
	int const subArrayTwo = right - mid;

	auto *leftArray = new int[subArrayOne],
		*rightArray = new int[subArrayTwo];

	for (auto i = 0; i < subArrayOne; i++)
		leftArray[i] = array[left + i];
	for (auto j = 0; j < subArrayTwo; j++)
		rightArray[j] = array[mid + 1 + j];

	auto indexOfSubArrayOne = 0, indexOfSubArrayTwo = 0;
	int indexOfMergedArray = left;

	while (indexOfSubArrayOne < subArrayOne
		&& indexOfSubArrayTwo < subArrayTwo) {
		if (leftArray[indexOfSubArrayOne]
			<= rightArray[indexOfSubArrayTwo]) {
			array[indexOfMergedArray]
				= leftArray[indexOfSubArrayOne];
			indexOfSubArrayOne++;
		}
		else {
			array[indexOfMergedArray]
				= rightArray[indexOfSubArrayTwo];
			indexOfSubArrayTwo++;
		}
		indexOfMergedArray++;
	}

	while (indexOfSubArrayOne < subArrayOne) {
		array[indexOfMergedArray]
			= leftArray[indexOfSubArrayOne];
		indexOfSubArrayOne++;
		indexOfMergedArray++;
	}

	while (indexOfSubArrayTwo < subArrayTwo) {
		array[indexOfMergedArray]
			= rightArray[indexOfSubArrayTwo];
		indexOfSubArrayTwo++;
		indexOfMergedArray++;
	}
	delete[] leftArray;
	delete[] rightArray;
}

void mergeSort(int array[], int const begin, int const end)
{
	if (begin >= end)
		return;

	int mid = begin + (end - begin) / 2;
	mergeSort(array, begin, mid);
	mergeSort(array, mid + 1, end);
	merge(array, begin, mid, end);
}

void printArray(int A[], int size)
{
	for (int i = 0; i < size; i++)
		cout << A[i] << " ";
	cout << endl;
}

int main()
{
	int arr[] = { 12, 11, 13, 5, 6, 7 };
	int arr_size = sizeof(arr) / sizeof(arr[0]);

	cout << "Given array is \n";
	printArray(arr, arr_size);

	mergeSort(arr, 0, arr_size - 1);

	cout << "\nSorted array is \n";
	printArray(arr, arr_size);
	return 0;
}
```

### $O(n^2)$ - Quadratic Time

Quadratic time complexity $O(n^2)$ indicates that the time taken by an algorithm is proportional to the square of the input size. It is common in algorithms that perform nested iterations over the data set.

Example: Bubble sort algorithm.

```cpp
#include <bits/stdc++.h>
using namespace std;

void bubbleSort(int arr[], int n)
{
	int i, j;
	bool swapped;
	for (i = 0; i < n - 1; i++) {
		swapped = false;
		for (j = 0; j < n - i - 1; j++) {
			if (arr[j] > arr[j + 1]) {
				swap(arr[j], arr[j + 1]);
				swapped = true;
			}
		}
		if (swapped == false)
			break;
	}
}

void printArray(int arr[], int size)
{
	int i;
	for (i = 0; i < size; i++)
		cout << " " << arr[i];
}

int main()
{
	int arr[] = { 64, 34, 25, 12, 22, 11, 90 };
	int N = sizeof(arr) / sizeof(arr[0]);
	bubbleSort(arr, N);
	cout << "Sorted array: \n";
	printArray(arr, N);
	return 0;
}
```

### $O(2^n)$ - Exponential Time

Exponential time complexity $O(2^n)$ describes an algorithm whose growth doubles with each addition to the input data set. This complexity is typical in brute-force algorithms for solving complex problems.

Example: Fibonacci sequence using recursion.

```cpp
#include <iostream>

int fibonacci(int n) {
    if (n <= 1)
        return n;
    return fibonacci(n - 1) + fibonacci(n - 2);
}

int main() {
    int n = 10;
    std::cout << "Fibonacci number is " << fibonacci(n);
    return 0;
}
```

_\* The detailed observation into the algorithms introduced in this part will be posted later_

## Visualization of the Time Complexities

```python
import matplotlib.pyplot as plt
import numpy as np

n = np.arange(1, 100)

plt.figure(figsize=(10, 6))

# O(1)
plt.plot(n, np.ones_like(n), label='$O(1)$', linestyle='--')

# O(n)
plt.plot(n, n, label='$O(n)$')

# O(log n)
plt.plot(n, np.log(n), label='$O(\log n)$')

# O(n log n)
plt.plot(n, n*np.log(n), label='$O(n \log n)$')

# O(n^2)
plt.plot(n, n**2, label='$O(n^2)$')

# O(2^n)
plt.plot(n, 2**n, label='$O(2^n)$')


plt.title('Common Time Complexities')
plt.xlabel('Input Size (n)')
plt.ylabel('Operations')
plt.grid(True)
plt.legend()
plt.yscale('log')
plt.ylim(0, 10000)

plt.show()
```

![Time Complexities Graph](assets/img/algorithm/BigOgraph.jpg)

## Conclusion

Analyzing the efficiency of an algorithm is a necessary process for making informed decisions when designing a program. Big O notation is a useful strategy for investigating the time and space complexity of an algorithm, and thus, understanding Big O notation is crucial for developers and programmers to optimize the performance of their programs.

### References

- Modeling Social Data. (2017, February 3). Lecture 3: Computational Complexity. Retrieved February 9, 2024, from [https://modelingsocialdata.org/lectures/2017/02/03/lecture-3-computational-complexity.html](https://modelingsocialdata.org/lectures/2017/02/03/lecture-3-computational-complexity.html)

- GeeksforGeeks. (n.d.). Binary Search. Retrieved February 9, 2024, from [https://www.geeksforgeeks.org/binary-search/](https://www.geeksforgeeks.org/binary-search/)

- GeeksforGeeks. (n.d.). Merge Sort. Retrieved February 9, 2024, from [https://www.geeksforgeeks.org/merge-sort/](https://www.geeksforgeeks.org/merge-sort/)

- GeeksforGeeks. (n.d.). Bubble Sort. Retrieved February 9, 2024, from [https://www.geeksforgeeks.org/bubble-sort/](https://www.geeksforgeeks.org/bubble-sort/)
